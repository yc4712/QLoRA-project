# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R4N20qCjbtMDFgtrbcFa4XDxKesieZdL
"""

# src/data_prep.py
import torch
from datasets import load_dataset, Dataset

def build_conversation_threads(dataset, tokenizer, max_seq_length: int):
    """
    Build conversation threads by selecting highest-ranked responses at each level.
    (This is your existing function from the notebook.)
    """
    messages = {ex["message_id"]: ex for ex in dataset}

    children_by_parent = {}
    for ex in dataset:
        parent_id = ex.get("parent_id")
        if parent_id:
            children_by_parent.setdefault(parent_id, []).append(ex)

    roots = [ex for ex in dataset if not ex.get("parent_id")]
    conversations = []

    def build_thread(message, conversation_parts):
        if message["role"] == "prompter":
            conversation_parts.append(f"### Human: {message['text']}\n ")
        else:
            conversation_parts.append(f"### Assistant: {message['text']}\n ")

        children = children_by_parent.get(message["message_id"])
        if not children:
            # Only keep if it ends with assistant and fits length
            if (
                message["role"] == "assistant"
                and len(
                    tokenizer.encode(
                        conversation_parts[0],
                        add_special_tokens=False,
                    )
                )
                + 2
                < max_seq_length
            ):
                conv_text = "".join(conversation_parts) + tokenizer.eos_token
                conversations.append(conv_text)
            return

        children_sorted = sorted(
            children,
            key=lambda x: x["rank"] if x.get("rank") else float("inf"),
        )
        best_child = children_sorted[0]
        build_thread(best_child, conversation_parts.copy())

    for root in roots:
        build_thread(root, [])

    return conversations


def preprocess_with_masking(example, tokenizer, max_seq_length: int):
    """
    Tokenize conversations and mask instruction parts.
    Only compute loss on assistant responses (not on human prompts).
    """
    human_marker = tokenizer.encode("### Human:", add_special_tokens=False)
    assistant_marker = tokenizer.encode("### Assistant:", add_special_tokens=False)

    tokenized = tokenizer(
        example["text"],
        truncation=True,
        max_length=max_seq_length,
        padding="max_length",
    )

    input_ids = tokenized["input_ids"]
    attention_mask = tokenized["attention_mask"]
    labels = [-100] * len(input_ids)

    i = 0
    while i < len(input_ids):
        if i + len(assistant_marker) <= len(input_ids):
            if input_ids[i : i + len(assistant_marker)] == assistant_marker:
                i += len(assistant_marker)
                while i < len(input_ids):
                    if (
                        i + len(human_marker) <= len(input_ids)
                        and input_ids[i : i + len(human_marker)] == human_marker
                    ):
                        break
                    if input_ids[i] == tokenizer.pad_token_id:
                        break
                    labels[i] = input_ids[i]
                    i += 1
        i += 1

    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels,
    }


def load_oasst1_splits(model_name: str, max_seq_length: int):
    """
    End-to-end helper:
    - loads OASST1
    - builds conversation threads
    - returns tokenized train/val datasets
    """
    dataset = load_dataset("OpenAssistant/oasst1")

    from transformers import AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    train_data = dataset["train"].select_columns(
        ["message_id", "parent_id", "text", "role", "rank"]
    )
    val_data = dataset["validation"].select_columns(
        ["message_id", "parent_id", "text", "role", "rank"]
    )

    train_conversations = build_conversation_threads(train_data, tokenizer, max_seq_length)
    val_conversations = build_conversation_threads(val_data, tokenizer, max_seq_length)

    train_dataset = Dataset.from_dict({"text": train_conversations})
    val_dataset = Dataset.from_dict({"text": val_conversations})

    tokenized_train = train_dataset.map(
        lambda ex: preprocess_with_masking(ex, tokenizer, max_seq_length),
        remove_columns=train_dataset.column_names,
        desc="Tokenizing train dataset",
    )
    tokenized_val = val_dataset.map(
        lambda ex: preprocess_with_masking(ex, tokenizer, max_seq_length),
        remove_columns=val_dataset.column_names,
        desc="Tokenizing validation dataset",
    )

    return tokenized_train, tokenized_val, tokenizer