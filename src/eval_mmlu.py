# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R4N20qCjbtMDFgtrbcFa4XDxKesieZdL
"""

# src/eval_mmlu.py
import os
import json
import argparse
import numpy as np
import torch
import torch.nn.functional as F
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel

LETTERS = ["A", "B", "C", "D"]

def format_example_5shot(ex):
    return ex["input"]

def score_choice(prompt, choice_letter, model, tokenizer):
    prompt_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
    choice_ids = tokenizer(" " + choice_letter, add_special_tokens=False).input_ids
    choice_ids = torch.tensor([choice_ids]).to(model.device)

    input_ids = torch.cat([prompt_ids, choice_ids], dim=1)

    with torch.no_grad():
        outputs = model(input_ids)
    logits = outputs.logits

    choice_len = choice_ids.shape[1]
    logits_for_choice = logits[:, -choice_len-1:-1, :]
    target_ids = choice_ids

    log_probs = F.log_softmax(logits_for_choice, dim=-1)
    token_log_probs = log_probs.gather(-1, target_ids.unsqueeze(-1)).squeeze(-1)
    return float(token_log_probs.sum().cpu())

def evaluate_mmlu_5shot(dataset, model, tokenizer, max_samples=None, verbose_every=50):
    n_correct = 0
    n_total = 0

    for i, ex in enumerate(dataset):
        if max_samples is not None and i >= max_samples:
            break

        prompt = format_example_5shot(ex)

        scores = [score_choice(prompt, letter, model, tokenizer) for letter in LETTERS]
        pred_idx = int(np.argmax(scores))
        pred_letter = LETTERS[pred_idx]

        gold_letter = str(ex["output"]).strip()[0]

        if pred_letter == gold_letter:
            n_correct += 1
        n_total += 1

        if verbose_every is not None and (i + 1) % verbose_every == 0:
            print(f"{i+1} examples, running accuracy = {n_correct / n_total:.4f}")

    return n_correct / n_total if n_total > 0 else 0.0

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--base_model", type=str, default="meta-llama/Llama-2-7b-hf")
    parser.add_argument("--adapter_dir_4bit", type=str, default="results/llama7b_4bit_qlora")
    parser.add_argument("--adapter_dir_16bit", type=str, default="results/llama7b_16bit_lora")
    parser.add_argument("--results_path", type=str, default="results/mmlu_scores.json")
    parser.add_argument("--mmlu_json", type=str, default="qlora/data/mmlu/five_shot_mmlu_test.json")
    parser.add_argument("--max_samples", type=int, default=None)
    args = parser.parse_args()

    mmlu = load_dataset("json", data_files={"test": args.mmlu_json})["test"]

    # 4-bit QLoRA
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
    )
    tokenizer = AutoTokenizer.from_pretrained(args.base_model, use_fast=False)
    tokenizer.pad_token = tokenizer.eos_token

    model_4bit = AutoModelForCausalLM.from_pretrained(
        args.base_model,
        quantization_config=bnb_config,
        device_map="auto",
    )
    model_4bit = PeftModel.from_pretrained(model_4bit, args.adapter_dir_4bit)
    model_4bit.eval()

    acc_4bit = evaluate_mmlu_5shot(mmlu, model_4bit, tokenizer, max_samples=args.max_samples)
    print(f"4-bit QLoRA accuracy: {acc_4bit:.4%}")

    # 16-bit LoRA
    model_16 = AutoModelForCausalLM.from_pretrained(
        args.base_model,
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )
    model_16 = PeftModel.from_pretrained(model_16, args.adapter_dir_16bit)
    model_16.eval()

    acc_16 = evaluate_mmlu_5shot(mmlu, model_16, tokenizer, max_samples=args.max_samples)
    print(f"16-bit LoRA accuracy: {acc_16:.4%}")

    os.makedirs(os.path.dirname(args.results_path), exist_ok=True)
    if os.path.exists(args.results_path):
        with open(args.results_path, "r") as f:
            scores = json.load(f)
    else:
        scores = {}
    scores["llama2_7b_4bit_qlora"] = {"mmlu_5shot_accuracy": float(acc_4bit)}
    scores["llama2_7b_16bit_lora"] = {"mmlu_5shot_accuracy": float(acc_16)}
    with open(args.results_path, "w") as f:
        json.dump(scores, f, indent=2)

if __name__ == "__main__":
    main()