{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook trains LLaMA-2-7B with both 4-bit QLoRA and 16-bit LoRA on OASST1, and compares training dynamics, memory usage, and evaluation loss."
      ],
      "metadata": {
        "id": "VidGXQo4J8P1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Setup & Dependencies"
      ],
      "metadata": {
        "id": "thSt04t8v3Rd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Zka71jYvlTY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    TrainerCallback,\n",
        "    Trainer\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    PeftModel\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Model Config: Shared params"
      ],
      "metadata": {
        "id": "pIbeDywozKpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
        "ADAPTER_DIR_4BIT = \"results/llama7b_4bit_qlora\"\n",
        "ADAPTER_DIR_16BIT = \"results/llama7b_16bit_lora\"\n",
        "\n",
        "OUTPUT_DIR_4BIT = \"./results/llama7b_4bit_qlora\"\n",
        "OUTPUT_DIR_16BIT = \"./results/llama7b_16bit_lora\"\n",
        "\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 4\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "LEARNING_RATE = 2e-4\n",
        "MAX_SEQ_LENGTH = 1024\n",
        "WARMUP_STEPS = 100\n",
        "LOGGING_STEPS = 10\n",
        "EVAL_STEPS = 50\n",
        "SAVE_STEPS = 100\n",
        "\n",
        "LORA_R = 64\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.1\n",
        "LORA_TARGET_MODULES = [\n",
        "    \"q_proj\",\n",
        "    \"k_proj\",\n",
        "    \"v_proj\",\n",
        "    \"o_proj\",\n",
        "    \"gate_proj\",\n",
        "    \"up_proj\",\n",
        "    \"down_proj\"\n",
        "]"
      ],
      "metadata": {
        "id": "QydlHushwEFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Load OASST1 Dataset + Prepare Conversation Trees + Mask Labels"
      ],
      "metadata": {
        "id": "APPRduOjwhVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "DATASET_NAME = \"OpenAssistant/oasst1\"\n",
        "dataset = load_dataset(DATASET_NAME)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Prepare Conversation Tree\n",
        "def build_conversation_threads(dataset):\n",
        "    \"\"\"\n",
        "    Build conversation threads by selecting highest-ranked responses at each level.\n",
        "    Walks down the tree from roots, choosing best child at each step.\n",
        "    \"\"\"\n",
        "    # Build lookup structures\n",
        "    messages = {ex['message_id']: ex for ex in dataset}\n",
        "\n",
        "    # Group children by parent_id\n",
        "    children_by_parent = {}\n",
        "    for ex in dataset:\n",
        "        parent_id = ex.get('parent_id')\n",
        "        if parent_id:\n",
        "            if parent_id not in children_by_parent:\n",
        "                children_by_parent[parent_id] = []\n",
        "            children_by_parent[parent_id].append(ex)\n",
        "\n",
        "    # Find root messages (no parent)\n",
        "    roots = [ex for ex in dataset if not ex.get('parent_id')]\n",
        "\n",
        "    conversations = []\n",
        "\n",
        "    def build_thread(message, conversation_parts):\n",
        "        \"\"\"Recursively build conversation by following best-ranked children\"\"\"\n",
        "        # Add current message to conversation\n",
        "        if message['role'] == 'prompter':\n",
        "            conversation_parts.append(f\"### Human: {message['text']}\\n \")\n",
        "        else:\n",
        "            conversation_parts.append(f\"### Assistant: {message['text']}\\n \")\n",
        "\n",
        "        # Get children of current message\n",
        "        children = children_by_parent.get(message['message_id'], None)\n",
        "\n",
        "        if not children:\n",
        "            # Leaf node - save conversation if it ends with assistant\n",
        "            if message['role'] == 'assistant' \\\n",
        "            and len(tokenizer.encode(conversation_parts[0],add_special_tokens=False))+len(tokenizer.encode('### Assistant: ',add_special_tokens=False))+2 < MAX_SEQ_LENGTH:\n",
        "                # Add assistant response\n",
        "                conv_text = ''.join(conversation_parts) + tokenizer.eos_token\n",
        "                conversations.append(conv_text)\n",
        "            return\n",
        "\n",
        "        # Sort children by rank (lower rank = better in OASST1)\n",
        "        # Handle missing ranks and None ranks\n",
        "        children_sorted = sorted(\n",
        "            children,\n",
        "            key=lambda x: x['rank'] if x.get('rank') else float('inf')\n",
        "        )\n",
        "\n",
        "        # Follow only the best-ranked child\n",
        "        best_child = children_sorted[0]\n",
        "        build_thread(best_child, conversation_parts.copy())\n",
        "\n",
        "    # Start building threads from each root\n",
        "    for root in roots:\n",
        "        build_thread(root, [])\n",
        "\n",
        "    return conversations\n",
        "\n",
        "def preprocess_with_masking(example):\n",
        "    \"\"\"\n",
        "    Tokenize conversations and mask instruction parts.\n",
        "    Only compute loss on assistant responses (not on human prompts).\n",
        "    \"\"\"\n",
        "\n",
        "    # Encode markers once for efficiency\n",
        "    human_marker = tokenizer.encode(\"### Human:\", add_special_tokens=False)\n",
        "    assistant_marker = tokenizer.encode(\"### Assistant:\", add_special_tokens=False)\n",
        "\n",
        "    # Tokenize full conversation\n",
        "    tokenized = tokenizer(\n",
        "        example['text'],\n",
        "        truncation=True,\n",
        "        max_length=MAX_SEQ_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    input_ids = tokenized['input_ids']\n",
        "    attention_mask = tokenized['attention_mask']\n",
        "\n",
        "    # Initialize labels - start by masking everything\n",
        "    labels = [-100] * len(input_ids)\n",
        "\n",
        "    # Find all positions where assistant responses start and end\n",
        "    i = 0\n",
        "    while i < len(input_ids):\n",
        "        # Look for \"### Assistant:\" marker\n",
        "        if i + len(assistant_marker) <= len(input_ids):\n",
        "            if input_ids[i:i+len(assistant_marker)] == assistant_marker:\n",
        "                # Skip past the marker itself (keep it masked)\n",
        "                i += len(assistant_marker)\n",
        "\n",
        "                # Unmask tokens until we hit \"### Human:\" or padding or end\n",
        "                while i < len(input_ids):\n",
        "                    # Check for \"### Human:\" marker\n",
        "                    if (i + len(human_marker) <= len(input_ids) and\n",
        "                        input_ids[i:i+len(human_marker)] == human_marker):\n",
        "                        break\n",
        "\n",
        "                    # Check for padding\n",
        "                    if input_ids[i] == tokenizer.pad_token_id:\n",
        "                        break\n",
        "\n",
        "                    # Unmask this token\n",
        "                    labels[i] = input_ids[i]\n",
        "                    i += 1\n",
        "        i += 1\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'labels': labels\n",
        "    }\n",
        "\n",
        "# training validation split\n",
        "train_data = dataset['train'].select_columns(['message_id', 'parent_id', 'text', 'role', 'rank'])\n",
        "val_data = dataset['validation'].select_columns(['message_id', 'parent_id', 'text', 'role', 'rank'])\n",
        "\n",
        "train_conversations = build_conversation_threads(train_data)\n",
        "val_conversations = build_conversation_threads(val_data)\n",
        "\n",
        "train_dataset = Dataset.from_dict({'text': train_conversations})\n",
        "val_dataset = Dataset.from_dict({'text': val_conversations})\n",
        "\n",
        "# Preprocess datasets\n",
        "tokenized_train = train_dataset.map(\n",
        "    preprocess_with_masking,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    desc=\"Tokenizing train dataset\"\n",
        ")\n",
        "\n",
        "tokenized_val = val_dataset.map(\n",
        "    preprocess_with_masking,\n",
        "    remove_columns=val_dataset.column_names,\n",
        "    desc=\"Tokenizing validation dataset\"\n",
        ")"
      ],
      "metadata": {
        "id": "DznzoKSRwzGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Memory and Performance Tracking"
      ],
      "metadata": {
        "id": "pA4ezTAuzhrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MemoryTracker:\n",
        "    \"\"\"Track GPU memory usage throughout training\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def update(self):\n",
        "        \"\"\"Update and return current memory usage\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            allocated = torch.cuda.memory_allocated() / 1e9\n",
        "            reserved = torch.cuda.memory_reserved() / 1e9\n",
        "            return allocated, reserved\n",
        "        return 0, 0\n",
        "\n",
        "    def get_peak_memory(self):\n",
        "        \"\"\"Get peak memory allocated since last reset\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            return torch.cuda.max_memory_allocated() / 1e9\n",
        "        return 0\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset peak memory stats and clear cache\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "class TrainingMetrics:\n",
        "    \"\"\"Store and compare training metrics\"\"\"\n",
        "    def __init__(self, method_name):\n",
        "        self.method_name = method_name\n",
        "        self.start_time = None\n",
        "        self.end_time = None\n",
        "        self.epoch_times = []\n",
        "        self.memory_tracker = MemoryTracker()\n",
        "        self.perplexity_history = []\n",
        "        self.loss_history = []\n",
        "\n",
        "    def start_training(self):\n",
        "        self.start_time = time.time()\n",
        "        self.memory_tracker.reset()\n",
        "\n",
        "    def end_training(self):\n",
        "        self.end_time = time.time()\n",
        "\n",
        "    def log_epoch(self, epoch_time, loss, perplexity):\n",
        "        self.epoch_times.append(epoch_time)\n",
        "        self.loss_history.append(loss)\n",
        "        self.perplexity_history.append(perplexity)\n",
        "\n",
        "    def get_summary(self):\n",
        "        total_time = self.end_time - self.start_time if self.end_time else 0\n",
        "        return {\n",
        "            'method': self.method_name,\n",
        "            'total_training_time_hours': total_time / 3600,\n",
        "            'avg_epoch_time_minutes': np.mean(self.epoch_times) / 60 if self.epoch_times else 0,\n",
        "            'peak_memory_gb': self.memory_tracker.get_peak_memory(),\n",
        "            'final_perplexity': self.perplexity_history[-1] if self.perplexity_history else None,\n",
        "            'final_loss': self.loss_history[-1] if self.loss_history else None\n",
        "        }\n",
        "\n",
        "def compute_perplexity(loss):\n",
        "    \"\"\"Convert loss to perplexity\"\"\"\n",
        "    return np.exp(loss)\n",
        "\n",
        "def get_model_size_mb(model_path):\n",
        "    \"\"\"Get total size of saved model in MB\"\"\"\n",
        "    total_size = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(model_path):\n",
        "        for f in filenames:\n",
        "            fp = os.path.join(dirpath, f)\n",
        "            total_size += os.path.getsize(fp)\n",
        "    return total_size / (1024 * 1024)\n",
        "\n",
        "class MetricsCallback(TrainerCallback):\n",
        "    def __init__(self, metrics_tracker):\n",
        "        self.metrics_tracker = metrics_tracker\n",
        "        self.epoch_start_time = None\n",
        "\n",
        "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
        "        self.epoch_start_time = time.time()\n",
        "        self.metrics_tracker.memory_tracker.update()\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        epoch_time = time.time() - self.epoch_start_time\n",
        "        # Get last logged loss and perplexity\n",
        "        if state.log_history:\n",
        "            last_log = state.log_history[-1]\n",
        "            loss = last_log.get('loss', 0)\n",
        "            perplexity = compute_perplexity(loss)\n",
        "            self.metrics_tracker.log_epoch(epoch_time, loss, perplexity)\n",
        "\n",
        "            print(f\"\\n{'─'*60}\")\n",
        "            print(f\"Epoch {state.epoch:.0f} Summary:\")\n",
        "            print(f\"  Time: {epoch_time/60:.2f} minutes\")\n",
        "            print(f\"  Loss: {loss:.4f}\")\n",
        "            print(f\"  Perplexity: {perplexity:.4f}\")\n",
        "            alloc, reserved = self.metrics_tracker.memory_tracker.update()\n",
        "            print(f\"  GPU Memory: {alloc:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
        "            print(f\"{'─'*60}\\n\")\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        self.metrics_tracker.memory_tracker.update()"
      ],
      "metadata": {
        "id": "-PAulM2BwFu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. 4-bit QLoRA fine-tuning"
      ],
      "metadata": {
        "id": "v1PJIUGxznAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create metric callback\n",
        "metrics_4bit = TrainingMetrics(\"4-bit QLORA\")\n",
        "metrics_callback = MetricsCallback(metrics_4bit)\n",
        "\n",
        "# Clear memory before loading\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "metrics_4bit.memory_tracker.reset()\n",
        "\n",
        "# Config for NF4 + DQ\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",  # NormalFloat4\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Load in Model\n",
        "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "model_4bit = prepare_model_for_kbit_training(model_4bit)\n",
        "\n",
        "lora_config_4bit = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=LORA_TARGET_MODULES,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Add QLoRA adapters\n",
        "model_4bit = get_peft_model(model_4bit, lora_config_4bit)"
      ],
      "metadata": {
        "id": "M2XL2I-jz-bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args_4_bit = TrainingArguments(\n",
        "        output_dir=OUTPUT_DIR_4BIT,\n",
        "        logging_dir=f\"{OUTPUT_DIR_4BIT}/logs\",\n",
        "        num_train_epochs=NUM_EPOCHS,\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        per_device_eval_batch_size=BATCH_SIZE,\n",
        "        eval_strategy='steps',\n",
        "        save_strategy=\"steps\",\n",
        "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        logging_steps=LOGGING_STEPS,\n",
        "        eval_steps=EVAL_STEPS,\n",
        "        save_steps=SAVE_STEPS,\n",
        "        warmup_steps=WARMUP_STEPS,\n",
        "        group_by_length=True,\n",
        "        max_grad_norm=0.3,\n",
        "        adam_beta2=0.999,\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        logging_first_step=True,\n",
        "        bf16 = True,\n",
        "        optim=\"paged_adamw_32bit\",  # Memory-efficient optimizer\n",
        "        gradient_checkpointing=True  # Save memory\n",
        "    )"
      ],
      "metadata": {
        "id": "TzvpkQ505YV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Trainer\n",
        "trainer_4bit = Trainer(\n",
        "    model=model_4bit,\n",
        "    args=training_args_4_bit,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    callbacks=[metrics_callback],\n",
        ")\n",
        "\n",
        "# Start training\n",
        "metrics_4bit.start_training()\n",
        "\n",
        "train_result = trainer_4bit.train()\n",
        "metrics_4bit.end_training()"
      ],
      "metadata": {
        "id": "4sEa126259W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. 16-bit LoRA fine-tuning"
      ],
      "metadata": {
        "id": "Oyq3UFqzzsUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create metric callback\n",
        "metrics_16bit = TrainingMetrics(\"16-bit LoRA\")\n",
        "metrics_callback = MetricsCallback(metrics_16bit)\n",
        "\n",
        "# Clear memory before loading\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "metrics_16bit.memory_tracker.reset()\n",
        "\n",
        "# Load in Model\n",
        "model_16bit = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "lora_config_16bit = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=LORA_TARGET_MODULES,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Add LoRA adapters\n",
        "model_16bit = get_peft_model(model_16bit, lora_config_16bit)"
      ],
      "metadata": {
        "id": "KidZff4N6V6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args_16_bit = TrainingArguments(\n",
        "        output_dir=OUTPUT_DIR_16BIT,\n",
        "        logging_dir=f\"{OUTPUT_DIR_16BIT}/logs\",\n",
        "        num_train_epochs=NUM_EPOCHS,\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        per_device_eval_batch_size=BATCH_SIZE,\n",
        "        eval_strategy='steps',\n",
        "        save_strategy=\"steps\",\n",
        "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        logging_steps=LOGGING_STEPS,\n",
        "        eval_steps=EVAL_STEPS,\n",
        "        save_steps=SAVE_STEPS,\n",
        "        warmup_steps=WARMUP_STEPS,\n",
        "        group_by_length=True,\n",
        "        max_grad_norm=0.3,\n",
        "        adam_beta2=0.999,\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        logging_first_step=True,\n",
        "        bf16 = True,\n",
        "        optim=\"adamw_torch\",\n",
        "        gradient_checkpointing=True\n",
        "    )"
      ],
      "metadata": {
        "id": "QzmnzOH362HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Trainer\n",
        "trainer_16bit = Trainer(\n",
        "    model=model_16bit,\n",
        "    args=training_args_16_bit,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    callbacks=[metrics_callback],\n",
        ")\n",
        "\n",
        "# Start training\n",
        "metrics_16bit.start_training()\n",
        "\n",
        "train_result = trainer_16bit.train()\n",
        "metrics_16bit.end_training()"
      ],
      "metadata": {
        "id": "SADljLW17Aef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Compare metrics + memory + timing"
      ],
      "metadata": {
        "id": "xgT3lUny7Dlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results_4bit = trainer_4bit.evaluate()\n",
        "eval_results_16bit = trainer_16bit.evaluate()\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"  4-bit Eval Loss: {eval_results_4bit['eval_loss']:.4f}\")\n",
        "print(f\"  4-bit Eval Perplexity: {compute_perplexity(eval_results_4bit['eval_loss']):.2f}\")\n",
        "print(f\"  16-bit Eval Loss: {eval_results_16bit['eval_loss']:.4f}\")\n",
        "print(f\"  16-bit Eval Perplexity: {compute_perplexity(eval_results_16bit['eval_loss']):.2f}\")\n",
        "\n",
        "print(\"\\nTraining Summary Report\")\n",
        "\n",
        "summary = metrics_4bit.get_summary()\n",
        "model_size_mb = get_model_size_mb(OUTPUT_DIR_4BIT)\n",
        "\n",
        "print(f\"\\nMethod: {summary['method']}\")\n",
        "print(f\"Total Training Time: {summary['total_training_time_hours']:.2f} hours\")\n",
        "print(f\"Average Epoch Time: {summary['avg_epoch_time_minutes']:.2f} minutes\")\n",
        "print(f\"Peak GPU Memory: {summary['peak_memory_gb']:.2f} GB\")\n",
        "print(f\"Final Loss: {summary['final_loss']:.4f}\")\n",
        "print(f\"Final Perplexity: {summary['final_perplexity']:.2f}\")\n",
        "print(f\"Model Size: {model_size_mb:.2f} MB ({model_size_mb/1024:.2f} GB)\")\n",
        "\n",
        "summary = metrics_16bit.get_summary()\n",
        "model_size_mb = get_model_size_mb(OUTPUT_DIR_16BIT)\n",
        "\n",
        "print(f\"\\nMethod: {summary['method']}\")\n",
        "print(f\"Total Training Time: {summary['total_training_time_hours']:.2f} hours\")\n",
        "print(f\"Average Epoch Time: {summary['avg_epoch_time_minutes']:.2f} minutes\")\n",
        "print(f\"Peak GPU Memory: {summary['peak_memory_gb']:.2f} GB\")\n",
        "print(f\"Final Loss: {summary['final_loss']:.4f}\")\n",
        "print(f\"Final Perplexity: {summary['final_perplexity']:.2f}\")\n",
        "print(f\"Model Size: {model_size_mb:.2f} MB ({model_size_mb/1024:.2f} GB)\")"
      ],
      "metadata": {
        "id": "--W1XqAw9VYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Save summaries"
      ],
      "metadata": {
        "id": "_tptAuiw7F8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_training_logs(trainer, metrics_tracker, output_dir):\n",
        "    \"\"\"\n",
        "    Save detailed training logs including:\n",
        "    - Full trainer log history (step-by-step)\n",
        "    - Epoch times\n",
        "    - Loss history per epoch\n",
        "    - Perplexity history per epoch\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # 1. Save trainer's complete log history (all steps)\n",
        "    if hasattr(trainer, 'state') and hasattr(trainer.state, 'log_history'):\n",
        "        log_history_file = os.path.join(output_dir, 'training_logs.json')\n",
        "        with open(log_history_file, 'w') as f:\n",
        "            json.dump(trainer.state.log_history, f, indent=2)\n",
        "        print(f\"✓ Saved training logs to {log_history_file}\")\n",
        "\n",
        "    # 2. Save epoch-level metrics\n",
        "    epoch_metrics = {\n",
        "        'epoch_times_seconds': metrics_tracker.epoch_times,\n",
        "        'epoch_times_minutes': [t/60 for t in metrics_tracker.epoch_times],\n",
        "        'loss_per_epoch': metrics_tracker.loss_history,\n",
        "        'perplexity_per_epoch': metrics_tracker.perplexity_history,\n",
        "    }\n",
        "\n",
        "    epoch_file = os.path.join(output_dir, 'epoch_metrics.json')\n",
        "    with open(epoch_file, 'w') as f:\n",
        "        json.dump(epoch_metrics, f, indent=2)\n",
        "    print(f\"✓ Saved epoch metrics to {epoch_file}\")\n",
        "\n",
        "    # 3. Save trainer state\n",
        "    if hasattr(trainer, 'state'):\n",
        "        trainer_state = {\n",
        "            'global_step': trainer.state.global_step,\n",
        "            'epoch': trainer.state.epoch,\n",
        "            'best_metric': trainer.state.best_metric,\n",
        "            'best_model_checkpoint': trainer.state.best_model_checkpoint,\n",
        "        }\n",
        "\n",
        "        state_file = os.path.join(output_dir, 'trainer_state.json')\n",
        "        with open(state_file, 'w') as f:\n",
        "            json.dump(trainer_state, f, indent=2)\n",
        "        print(f\"✓ Saved trainer state to {state_file}\")\n",
        "\n",
        "\n",
        "def save_evaluation_results(trainer, output_dir):\n",
        "    \"\"\"\n",
        "    Save final evaluation results\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        eval_results = trainer.evaluate()\n",
        "\n",
        "        eval_file = os.path.join(output_dir, 'evaluation_results.json')\n",
        "        with open(eval_file, 'w') as f:\n",
        "            json.dump(eval_results, f, indent=2)\n",
        "\n",
        "        print(f\"✓ Saved evaluation results to {eval_file}\")\n",
        "        print(f\"  Final eval loss: {eval_results.get('eval_loss', 'N/A')}\")\n",
        "\n",
        "        return eval_results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error saving evaluation results: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "lCLzPeuqGu8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_training_logs(trainer_4bit, metrics_4bit, OUTPUT_DIR_4BIT)\n",
        "save_evaluation_results(trainer_4bit, OUTPUT_DIR_4BIT)\n",
        "\n",
        "save_training_logs(trainer_16bit, metrics_16bit, OUTPUT_DIR_16BIT)\n",
        "save_evaluation_results(trainer_16bit, OUTPUT_DIR_16BIT)"
      ],
      "metadata": {
        "id": "PjX9gzFfAzF3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}