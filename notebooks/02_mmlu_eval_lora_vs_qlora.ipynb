{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MMLU Evaluation â€“ LLaMA-2-7B LoRA vs QLoRA\n",
        "\n",
        "This notebook evaluates two fine-tuned models:\n",
        "\n",
        "- 4-bit QLoRA on LLaMA-2-7B\n",
        "- 16-bit LoRA on LLaMA-2-7B\n",
        "\n",
        "We use the 5-shot MMLU benchmark (multiple-choice), and compare accuracy across the same test split.\n"
      ],
      "metadata": {
        "id": "fBb22hbcVyCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Setup & Dependencies"
      ],
      "metadata": {
        "id": "gR4fTc-ZWFLA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ38RLI9UrRr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import torch.nn.functional as F\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Configurations"
      ],
      "metadata": {
        "id": "_xFnLjS1WWEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "ADAPTER_DIR_4BIT  = \"results/llama7b_4bit_qlora\"\n",
        "ADAPTER_DIR_16BIT = \"results/llama7b_16bit_lora\"\n",
        "\n",
        "# Where to save the final scores\n",
        "MMLU_RESULTS_PATH = \"results/mmlu_scores.json\"\n",
        "\n",
        "# If you're using the qlora repo JSONs:\n",
        "#MMLU_JSON_DIR = \"data/mmlu\"  # e.g. after cloning / downloading qlora data\n",
        "#MMLU_JSON_FILE = \"five_shot_mmlu_test.json\""
      ],
      "metadata": {
        "id": "8ZTX742HV5JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Load 5-shot MMLU dataset"
      ],
      "metadata": {
        "id": "QqTEk1rZWerE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/artidoro/qlora.git\n",
        "MMLU_DIR = \"qlora/data/mmlu\"\n",
        "\n",
        "mmlu = load_dataset(\n",
        "    \"json\",\n",
        "    data_files={\"test\": f\"{MMLU_DIR}/five_shot_mmlu_test.json\"},\n",
        ")[\"test\"]\n",
        "\n",
        "print(\"MMLU 5-shot test size:\", len(mmlu))\n",
        "print(mmlu[0])"
      ],
      "metadata": {
        "id": "BrLA0LdlWdZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Scoring functions"
      ],
      "metadata": {
        "id": "AmCmPxbrWpon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LETTERS = [\"A\", \"B\", \"C\", \"D\"]\n",
        "\n",
        "def format_example_5shot(ex):\n",
        "    return ex[\"input\"]\n",
        "\n",
        "def score_choice(prompt, choice_letter):\n",
        "    \"\"\"\n",
        "    Compute log-probability of the answer letter (\" A\"/\" B\"/...) given the prompt.\n",
        "    \"\"\"\n",
        "    prompt_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "    choice_ids = tokenizer(\" \" + choice_letter, add_special_tokens=False).input_ids\n",
        "    choice_ids = torch.tensor([choice_ids]).to(model.device)\n",
        "\n",
        "    input_ids = torch.cat([prompt_ids, choice_ids], dim=1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    choice_len = choice_ids.shape[1]\n",
        "    logits_for_choice = logits[:, -choice_len-1:-1, :]\n",
        "    target_ids = choice_ids\n",
        "\n",
        "    log_probs = F.log_softmax(logits_for_choice, dim=-1)\n",
        "    token_log_probs = log_probs.gather(-1, target_ids.unsqueeze(-1)).squeeze(-1)\n",
        "    return float(token_log_probs.sum().cpu())\n",
        "\n",
        "def evaluate_mmlu_5shot(dataset, max_samples=None, verbose_every=50):\n",
        "    n_correct = 0\n",
        "    n_total = 0\n",
        "\n",
        "    for i, ex in enumerate(dataset):\n",
        "        if max_samples is not None and i >= max_samples:\n",
        "            break\n",
        "\n",
        "        prompt = format_example_5shot(ex)\n",
        "\n",
        "        scores = []\n",
        "        for letter in LETTERS:\n",
        "            scores.append(score_choice(prompt, letter))\n",
        "\n",
        "        pred_idx = int(np.argmax(scores))\n",
        "        pred_letter = LETTERS[pred_idx]\n",
        "\n",
        "        gold_letter = str(ex[\"output\"]).strip()[0]\n",
        "\n",
        "        if pred_letter == gold_letter:\n",
        "            n_correct += 1\n",
        "        n_total += 1\n",
        "\n",
        "        if verbose_every is not None and (i + 1) % verbose_every == 0:\n",
        "            print(f\"{i+1} examples, running accuracy = {n_correct / n_total:.4f}\")\n",
        "\n",
        "    acc = n_correct / n_total if n_total > 0 else 0.0\n",
        "    return acc"
      ],
      "metadata": {
        "id": "hhQ53uE-WxTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Evaluate 4-bit QLoRA"
      ],
      "metadata": {
        "id": "r9VkjmaPXBOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=False)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# base model in 4-bit\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# attach QLoRA adapter\n",
        "model = PeftModel.from_pretrained(model, ADAPTER_DIR_4BIT)\n",
        "model.eval()\n",
        "\n",
        "# run evaluation\n",
        "acc_4bit = evaluate_mmlu_5shot(mmlu, max_samples=None)\n",
        "print(f\"4-bit QLoRA MMLU 5-shot accuracy: {acc_4bit:.4%}\")"
      ],
      "metadata": {
        "id": "S8z65imxXD98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Evaluate 16-bit LoRA"
      ],
      "metadata": {
        "id": "95XCFjEzXIUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reuse tokenizer (same base model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=False)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# base model in bf16\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# attach LoRA adapter\n",
        "model = PeftModel.from_pretrained(model, ADAPTER_DIR_16BIT)\n",
        "model.eval()\n",
        "\n",
        "acc_16bit = evaluate_mmlu_5shot(mmlu, max_samples=None)\n",
        "print(f\"16-bit LoRA MMLU 5-shot accuracy: {acc_16bit:.4%}\")"
      ],
      "metadata": {
        "id": "T-E1eWTnXLhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Compare & Save Results"
      ],
      "metadata": {
        "id": "BehkQ0yAXPyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(os.path.dirname(MMLU_RESULTS_PATH), exist_ok=True)\n",
        "\n",
        "# load existing scores file if present\n",
        "if os.path.exists(MMLU_RESULTS_PATH):\n",
        "    with open(MMLU_RESULTS_PATH, \"r\") as f:\n",
        "        scores = json.load(f)\n",
        "else:\n",
        "    scores = {}\n",
        "\n",
        "scores[\"llama2_7b_4bit_qlora\"] = {\n",
        "    \"mmlu_5shot_accuracy\": float(acc_4bit),\n",
        "}\n",
        "scores[\"llama2_7b_16bit_lora\"] = {\n",
        "    \"mmlu_5shot_accuracy\": float(acc_16bit),\n",
        "}\n",
        "\n",
        "with open(MMLU_RESULTS_PATH, \"w\") as f:\n",
        "    json.dump(scores, f, indent=2)\n",
        "\n",
        "print(\"\\nSaved MMLU scores to\", MMLU_RESULTS_PATH)\n",
        "print(json.dumps(scores, indent=2))"
      ],
      "metadata": {
        "id": "QWGcMGoQXau1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}